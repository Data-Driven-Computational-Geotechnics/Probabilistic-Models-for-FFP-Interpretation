{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM41JgLnq1EXrSDhTIV3s59"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"nWRPpOrq89Bt"},"outputs":[],"source":["# =========================\n","# 0) Imports & Setup\n","# =========================\n","import os\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import (\n","    r2_score,\n","    mean_absolute_percentage_error,\n","    mean_squared_error,\n","    mean_absolute_error\n",")\n","\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","from google.colab import drive\n","import openpyxl\n","tfd = tfp.distributions"]},{"cell_type":"code","source":["# -------------------------\n","# Reproducibility (optional)\n","# -------------------------\n","SEED = 42\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)"],"metadata":{"id":"GWFC5s969JNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 1) Mount Drive & Load Data\n","# =========================\n","drive.mount('/content/drive')\n","\n","# Input dataset path (Excel)\n","FILE_PATH = '/content/drive/My Drive/Objective1/ANN/FFP_Data.xlsx'\n","\n","# Load data\n","data = pd.read_excel(FILE_PATH)\n","\n","# Split into features (X) and target (y)\n","X = data.iloc[:, :-1].values\n","y = data.iloc[:, -1].values"],"metadata":{"id":"XJEg8BY99Oqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 2) Variational Building Blocks\n","# =========================\n","negloglik = lambda y_true, rv_y: -rv_y.log_prob(y_true)\n","\n","def prior_trainable(kernel_size, bias_size=0, dtype=None):\n","    n = kernel_size + bias_size\n","    return tf.keras.Sequential([\n","        tfp.layers.VariableLayer(n, dtype=dtype),\n","        tfp.layers.DistributionLambda(\n","            lambda t: tfd.Independent(\n","                tfd.Normal(loc=t, scale=1.0),\n","                reinterpreted_batch_ndims=1\n","            )\n","        ),\n","    ])\n","\n","def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n","    n = kernel_size + bias_size\n","    c = np.log(np.expm1(1.0))\n","    return tf.keras.Sequential([\n","        tfp.layers.VariableLayer(2 * n, dtype=dtype),\n","        tfp.layers.DistributionLambda(\n","            lambda t: tfd.Independent(\n","                tfd.Normal(\n","                    loc=t[..., :n],\n","                    scale=1e-5 + tf.nn.softplus(c + t[..., n:])\n","                ),\n","                reinterpreted_batch_ndims=1\n","            )\n","        ),\n","    ])"],"metadata":{"id":"TUgkXB3Q9RAb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 3) I/O Paths (Excel Outputs)\n","# =========================\n","METRICS_XLSX = '/content/drive/My Drive/Optimization of BNN-VI/Metrics_VI_BNN.xlsx'\n","TRAIN_PRED_XLSX = '/content/drive/My Drive/Optimization of BNN-VI/Training_Predictions_VI_BNN.xlsx'\n","TEST_PRED_XLSX  = '/content/drive/My Drive/Optimization of BNN-VI/Testing_Predictions_VI_BNN.xlsx'\n","\n","\n","for path in [TRAIN_PRED_XLSX, TEST_PRED_XLSX]:\n","    if not os.path.exists(path):\n","        with pd.ExcelWriter(path, engine='openpyxl') as writer:\n","            pd.DataFrame({\"Init\": []}).to_excel(writer, sheet_name='Init', index=False)\n","\n","if not os.path.exists(METRICS_XLSX):\n","    with pd.ExcelWriter(METRICS_XLSX, engine='openpyxl') as writer:\n","        pd.DataFrame(columns=[\n","            'Iteration',\n","            'R2_Train', 'R2_Test',\n","            'MAPE_Train', 'MAPE_Test',\n","            'MAE_Train', 'MAE_Test',\n","            'RMSE_Train', 'RMSE_Test'\n","        ]).to_excel(writer, index=False)"],"metadata":{"id":"qlnfqFsp9eRZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 4) Experiment Config\n","# =========================\n","iterations  = 30\n","mc_samples  = 5000\n","\n","\n","metrics_list = []"],"metadata":{"id":"iVcZPqJS9mJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 5) Main Loop: Split ‚Üí Scale ‚Üí Build ‚Üí Train ‚Üí Sample ‚Üí Save\n","# =========================\n","for iteration in range(iterations):\n","    print(f\"üîÅ Iteration {iteration + 1}/{iterations}\")\n","\n","    # -------------------------\n","    # 5.1) Train/Val/Test Split\n","    # -------------------------\n","    X_train, X_temp, y_train, y_temp = train_test_split(\n","        X, y, test_size=0.3, random_state=iteration\n","    )\n","    X_val, X_test, y_val, y_test = train_test_split(\n","        X_temp, y_temp, test_size=0.5, random_state=iteration\n","    )\n","\n","    # -------------------------\n","    # 5.2) Scaling (fit on train only)\n","    # -------------------------\n","    scaler = StandardScaler()\n","    X_train = scaler.fit_transform(X_train)\n","    X_val   = scaler.transform(X_val)\n","    X_test  = scaler.transform(X_test)\n","\n","    # -------------------------\n","    # 5.3) Build VI-BNN\n","    # -------------------------\n","    N_train   = X_train.shape[0]\n","    kl_weight = 1.0 / float(N_train)\n","\n","    model = tf.keras.Sequential([\n","        # Hidden layer 1 (variational Dense)\n","        tfp.layers.DenseVariational(\n","            units=30,\n","            make_posterior_fn=posterior_mean_field,\n","            make_prior_fn=prior_trainable,\n","            kl_weight=kl_weight,\n","            activation='relu'\n","        ),\n","        # Hidden layer 2 (variational Dense)\n","        tfp.layers.DenseVariational(\n","            units=20,\n","            make_posterior_fn=posterior_mean_field,\n","            make_prior_fn=prior_trainable,\n","            kl_weight=kl_weight,\n","            activation='relu'\n","        ),\n","        tfp.layers.DenseVariational(\n","            units=2,\n","            make_posterior_fn=posterior_mean_field,\n","            make_prior_fn=prior_trainable,\n","            kl_weight=kl_weight\n","        ),\n","        tfp.layers.DistributionLambda(\n","            lambda t: tfd.Normal(\n","                loc=t[..., :1],                                   # predictive mean\n","                scale=1e-3 + tf.math.softplus(0.01 * t[..., 1:])  # predictive std (aleatoric)\n","            )\n","        )\n","    ])\n","\n","    # -------------------------\n","    # 5.4) Compile & Train\n","    # -------------------------\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n","        loss=negloglik\n","    )\n","\n","    model.fit(\n","        X_train, y_train,\n","        epochs=1000,\n","        verbose=False,\n","        validation_data=(X_val, y_val)\n","    )\n","\n","    # -------------------------\n","    # 5.5) Monte Carlo Predictive Sampling\n","    # -------------------------\n","    train_means, test_means = [], []\n","    train_stds,  test_stds  = [], []\n","    train_samples, test_samples = [], []\n","\n","    for _ in range(mc_samples):\n","        pred_train = model(X_train)\n","        pred_test  = model(X_test)\n","\n","        mu_tr = pred_train.mean().numpy().flatten()\n","        sd_tr = pred_train.stddev().numpy().flatten()\n","\n","        mu_te = pred_test.mean().numpy().flatten()\n","        sd_te = pred_test.stddev().numpy().flatten()\n","\n","        samp_tr = np.random.normal(loc=mu_tr, scale=sd_tr)\n","        samp_te = np.random.normal(loc=mu_te, scale=sd_te)\n","\n","        train_means.append(mu_tr)\n","        train_stds.append(sd_tr)\n","        train_samples.append(samp_tr)\n","\n","        test_means.append(mu_te)\n","        test_stds.append(sd_te)\n","        test_samples.append(samp_te)\n","\n","    # -------------------------\n","    # 5.6) Aggregate Predictions & Uncertainty\n","    # -------------------------\n","    mean_train = np.mean(train_means, axis=0)\n","    mean_test  = np.mean(test_means,  axis=0)\n","\n","    epistemic_std_train = np.std(train_means, axis=0, ddof=1)\n","    epistemic_std_test  = np.std(test_means,  axis=0, ddof=1)\n","\n","    aleatoric_std_train = np.sqrt(np.mean(np.square(np.array(train_stds)), axis=0))\n","    aleatoric_std_test  = np.sqrt(np.mean(np.square(np.array(test_stds)),  axis=0))\n","\n","    lower_train = np.percentile(train_samples, 2.5, axis=0)\n","    upper_train = np.percentile(train_samples, 97.5, axis=0)\n","    lower_test  = np.percentile(test_samples, 2.5, axis=0)\n","    upper_test  = np.percentile(test_samples, 97.5, axis=0)\n","\n","    # -------------------------\n","    # 5.7) Metrics (Train/Test)\n","    # -------------------------\n","    metrics_row = {\n","        'Iteration': iteration + 1,\n","        'R2_Train':  r2_score(y_train, mean_train),\n","        'R2_Test':   r2_score(y_test,  mean_test),\n","        'MAPE_Train': mean_absolute_percentage_error(y_train, mean_train),\n","        'MAPE_Test':  mean_absolute_percentage_error(y_test,  mean_test),\n","        'MAE_Train':  mean_absolute_error(y_train, mean_train),\n","        'MAE_Test':   mean_absolute_error(y_test,  mean_test),\n","        'RMSE_Train': np.sqrt(mean_squared_error(y_train, mean_train)),\n","        'RMSE_Test':  np.sqrt(mean_squared_error(y_test,  mean_test)),\n","    }\n","    metrics_list.append(metrics_row)\n","\n","\n","    pd.DataFrame(metrics_list).to_excel(METRICS_XLSX, index=False)\n","\n","    # -------------------------\n","    # 5.8) Save Predictions to Excel (per-iteration sheet)\n","    # -------------------------\n","    # Training predictions\n","    train_df = pd.DataFrame({\n","        'Actual': y_train.flatten(),\n","        'Predicted_Mean': mean_train,\n","        'Epistemic_Std': epistemic_std_train,\n","        'Aleatoric_Std': aleatoric_std_train,\n","        'Lower_95CI': lower_train,\n","        'Upper_95CI': upper_train\n","    })\n","    with pd.ExcelWriter(TRAIN_PRED_XLSX, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n","        train_df.to_excel(writer, sheet_name=f'Iter_{iteration + 1}', index=False)\n","\n","    # Testing predictions\n","    test_df = pd.DataFrame({\n","        'Actual': y_test.flatten(),\n","        'Predicted_Mean': mean_test,\n","        'Epistemic_Std': epistemic_std_test,\n","        'Aleatoric_Std': aleatoric_std_test,\n","        'Lower_95CI': lower_test,\n","        'Upper_95CI': upper_test\n","    })\n","    with pd.ExcelWriter(TEST_PRED_XLSX, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n","        test_df.to_excel(writer, sheet_name=f'Iter_{iteration + 1}', index=False)"],"metadata":{"id":"NVoDI5Nc9o2Y"},"execution_count":null,"outputs":[]}]}