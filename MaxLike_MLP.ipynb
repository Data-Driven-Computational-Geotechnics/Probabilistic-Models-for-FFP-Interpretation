{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZF38xoa42ei"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# 1) IMPORTS\n",
        "# ===============================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    r2_score, mean_absolute_percentage_error,\n",
        "    mean_squared_error, mean_absolute_error\n",
        ")\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import openpyxl  # engine for Excel I/O\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 2) MOUNT GOOGLE DRIVE\n",
        "# ===============================================================\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Qq8Y-_pr5_Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 3) LOAD DATA\n",
        "# ===============================================================\n",
        "file_path = '/content/drive/My Drive/Objective1/FFP_Data.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Features & target\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "Kc2VNEiF5_XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 4) SETUP: OUTPUT PATHS, LOSS, AND CONTAINERS\n",
        "# ===============================================================\n",
        "iterations = 30  # number of random-subsampling runs\n",
        "\n",
        "metrics_file_path = '/content/drive/My Drive/Objective1/Maxlike_ANN/Metrics_Maxlike_ANN_2.xlsx'\n",
        "training_predictions_file_path = '/content/drive/My Drive/Objective1/Maxlike_ANN/Training_Predictions_Maxlike_ANN.xlsx'\n",
        "testing_predictions_file_path  = '/content/drive/My Drive/Objective1/Maxlike_ANN/Testing_Predictions_Maxlike_ANN.xlsx'\n",
        "\n",
        "tfd = tfp.distributions\n",
        "negloglik = lambda y_true, rv_y: -rv_y.log_prob(y_true)  # Normal NLL\n",
        "\n",
        "metrics = []\n",
        "all_losses = []"
      ],
      "metadata": {
        "id": "prlMbLsp5_fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 5) TRAIN/EVAL LOOP (REPEATED RANDOM SUBSAMPLING)\n",
        "# ===============================================================\n",
        "with pd.ExcelWriter(training_predictions_file_path, engine='openpyxl') as train_writer, \\\n",
        "     pd.ExcelWriter(testing_predictions_file_path,  engine='openpyxl') as test_writer:\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        print(f'Iteration {iteration + 1}/{iterations}')\n",
        "\n",
        "        # -----------------------------\n",
        "        # 5.1 Splits: Train / Val / Test\n",
        "        # -----------------------------\n",
        "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "            X, y, test_size=0.30, random_state=iteration\n",
        "        )\n",
        "        X_val, X_test, y_val, y_test = train_test_split(\n",
        "            X_temp, y_temp, test_size=0.50, random_state=iteration\n",
        "        )\n",
        "\n",
        "        # -----------------------------\n",
        "        # 5.2 Scale features (fit on train)\n",
        "        # -----------------------------\n",
        "        scaler = StandardScaler()\n",
        "        X_train_s = scaler.fit_transform(X_train)\n",
        "        X_val_s   = scaler.transform(X_val)\n",
        "        X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "        # -----------------------------\n",
        "        # 5.3 Build probabilistic model\n",
        "        # -----------------------------\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(30, activation='relu', input_shape=(X_train_s.shape[1],)),\n",
        "            tf.keras.layers.Dense(20, activation='relu'),\n",
        "            tf.keras.layers.Dense(2),  # params for loc & (pre-activation) scale\n",
        "            tfp.layers.DistributionLambda(\n",
        "                lambda t: tfd.Normal(\n",
        "                    loc=t[..., :1],\n",
        "                    scale=1e-3 + tf.math.softplus(0.01 * t[..., 1:])\n",
        "                )\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        # -----------------------------\n",
        "        # 5.4 Compile & Train\n",
        "        # -----------------------------\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "            loss=negloglik\n",
        "        )\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train_s, y_train,\n",
        "            epochs=1000,\n",
        "            verbose=False,\n",
        "            validation_data=(X_val_s, y_val)\n",
        "        )\n",
        "\n",
        "        # Track loss history\n",
        "        all_losses.append(history.history['loss'])\n",
        "\n",
        "        # -----------------------------\n",
        "        # 5.5 Plot training/validation loss\n",
        "        # -----------------------------\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title(f'Loss vs Epoch â€” Iteration {iteration + 1}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Negative Log-Likelihood')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        # -----------------------------\n",
        "        # 5.6 Predictions (mean, std, 95% CI)\n",
        "        # -----------------------------\n",
        "        yhat_train = model(X_train_s)\n",
        "        yhat_test  = model(X_test_s)\n",
        "\n",
        "        mean_train  = yhat_train.mean().numpy().flatten()\n",
        "        std_train   = yhat_train.stddev().numpy().flatten()\n",
        "        lower_train = mean_train - 1.96 * std_train\n",
        "        upper_train = mean_train + 1.96 * std_train\n",
        "\n",
        "        mean_test  = yhat_test.mean().numpy().flatten()\n",
        "        std_test   = yhat_test.stddev().numpy().flatten()\n",
        "        lower_test = mean_test - 1.96 * std_test\n",
        "        upper_test = mean_test + 1.96 * std_test\n",
        "\n",
        "        # -----------------------------\n",
        "        # 5.7 Metrics (train & test)\n",
        "        # -----------------------------\n",
        "        metrics.append({\n",
        "            'Iteration': iteration + 1,\n",
        "            'R2_Train':  r2_score(y_train, mean_train),\n",
        "            'R2_Test':   r2_score(y_test,  mean_test),\n",
        "            'MAPE_Train': mean_absolute_percentage_error(y_train, mean_train),\n",
        "            'MAPE_Test':  mean_absolute_percentage_error(y_test,  mean_test),\n",
        "            'MAE_Train':  mean_absolute_error(y_train, mean_train),\n",
        "            'MAE_Test':   mean_absolute_error(y_test,  mean_test),\n",
        "            'RMSE_Train': np.sqrt(mean_squared_error(y_train, mean_train)),\n",
        "            'RMSE_Test':  np.sqrt(mean_squared_error(y_test,  mean_test)),\n",
        "        })\n",
        "\n",
        "        # -----------------------------\n",
        "        # 5.8 Save per-iteration predictions to Excel\n",
        "        # -----------------------------\n",
        "        train_results = pd.DataFrame({\n",
        "            'Actual': y_train.flatten(),\n",
        "            'Predicted_Mean': mean_train,\n",
        "            'Predicted_StdDev': std_train,\n",
        "            'Lower_95CI': lower_train,\n",
        "            'Upper_95CI': upper_train\n",
        "        })\n",
        "        train_results.to_excel(train_writer, sheet_name=f'Iteration_{iteration + 1}', index=False)\n",
        "\n",
        "        test_results = pd.DataFrame({\n",
        "            'Actual': y_test.flatten(),\n",
        "            'Predicted_Mean': mean_test,\n",
        "            'Predicted_StdDev': std_test,\n",
        "            'Lower_95CI': lower_test,\n",
        "            'Upper_95CI': upper_test\n",
        "        })\n",
        "        test_results.to_excel(test_writer, sheet_name=f'Iteration_{iteration + 1}', index=False)"
      ],
      "metadata": {
        "id": "_fVLt6cz7Iz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 6) SAVE METRICS SUMMARY\n",
        "# ===============================================================\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "metrics_df.to_excel(metrics_file_path, index=False)"
      ],
      "metadata": {
        "id": "T7SDcU5V7o59"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}